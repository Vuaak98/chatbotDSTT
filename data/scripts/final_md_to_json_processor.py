#!/usr/bin/env python3
"""
X·ª≠ l√Ω k·ª∑ y·∫øu ƒë·∫°i s·ªë tuy·∫øn t√≠nh: Chuy·ªÉn ƒë·ªïi MD sang JSON
- Ngu·ªìn: K·ª∑ y·∫øu ƒë·∫°i s·ªë tuy·∫øn t√≠nh c√°c nƒÉm (PDF ‚Üí Mathpix ‚Üí MD)
- C·∫•u tr√∫c: 2 lo·∫°i ƒë·ªÅ thi (bangA, bangB) + 7 d·∫°ng b√†i t·∫≠p
- Output: JSON t·ªëi ∆∞u cho RAG system
"""

import os
import re
import json
import logging
import datetime
import itertools
import unicodedata
import uuid

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("final_md_to_json_processor.log", mode='w', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# ========================
# CATEGORY MAPPING - H·ªá th·ªëng ph√¢n lo·∫°i 2 c·∫•p ho√†n ch·ªânh
# ========================

CATEGORY_MAPPING = {
    # ƒê·ªÅ thi Olympic
    "dethi_bangA": {
        "category": "dethi",
        "subcategory": "bangA",
        "display_name": "ƒê·ªÅ thi Olympic B·∫£ng A",
        "description": "C√°c ƒë·ªÅ thi Olympic To√°n h·ªçc sinh vi√™n b·∫£ng A - d√†nh cho sinh vi√™n nƒÉm 1, 2",
        "difficulty_level": "quoc_gia",
        "keywords": ["ƒë·ªÅ thi", "thi", "olympic", "thi ƒë·∫•u", "b·∫£ng A", "bang A", "b·∫£ng a"]
    },
    "dethi_bangB": {
        "category": "dethi", 
        "subcategory": "bangB",
        "display_name": "ƒê·ªÅ thi Olympic B·∫£ng B",
        "description": "C√°c ƒë·ªÅ thi Olympic To√°n h·ªçc sinh vi√™n b·∫£ng B - d√†nh cho sinh vi√™n nƒÉm 3, 4",
        "difficulty_level": "quoc_gia",
        "keywords": ["ƒë·ªÅ thi", "thi", "olympic", "thi ƒë·∫•u", "b·∫£ng B", "bang B", "b·∫£ng b"]
    },
    
    # 7 d·∫°ng b√†i t·∫≠p
    "GTR": {
        "category": "baitap",
        "subcategory": "gtr", 
        "display_name": "Gi√° tr·ªã ri√™ng - Vector ri√™ng",
        "description": "B√†i t·∫≠p v·ªÅ gi√° tr·ªã ri√™ng v√† vector ri√™ng c·ªßa ma tr·∫≠n",
        "difficulty_level": "olympic",
        "keywords": ["b√†i t·∫≠p", "luy·ªán t·∫≠p", "d·∫°ng b√†i", "gi√° tr·ªã ri√™ng", "vector ri√™ng", "eigenvalue", "eigenvector"]
    },
    "HPT": {
        "category": "baitap",
        "subcategory": "hpt",
        "display_name": "H·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh", 
        "description": "B√†i t·∫≠p v·ªÅ h·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh v√† ph∆∞∆°ng ph√°p gi·∫£i",
        "difficulty_level": "olympic",
        "keywords": ["b√†i t·∫≠p", "luy·ªán t·∫≠p", "d·∫°ng b√†i", "h·ªá ph∆∞∆°ng tr√¨nh", "ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh", "system"]
    },
    "KGVT": {
        "category": "baitap",
        "subcategory": "kgvt",
        "display_name": "Kh√¥ng gian vector",
        "description": "B√†i t·∫≠p v·ªÅ kh√¥ng gian vector v√† √°nh x·∫° tuy·∫øn t√≠nh",
        "difficulty_level": "olympic", 
        "keywords": ["b√†i t·∫≠p", "luy·ªán t·∫≠p", "d·∫°ng b√†i", "kh√¥ng gian vector", "√°nh x·∫° tuy·∫øn t√≠nh", "vector space"]
    },
    "MT": {
        "category": "baitap",
        "subcategory": "mt",
        "display_name": "Ma tr·∫≠n",
        "description": "B√†i t·∫≠p v·ªÅ ma tr·∫≠n v√† c√°c ph√©p to√°n ma tr·∫≠n",
        "difficulty_level": "olympic",
        "keywords": ["b√†i t·∫≠p", "luy·ªán t·∫≠p", "d·∫°ng b√†i", "ma tr·∫≠n", "matrix", "ph√©p to√°n ma tr·∫≠n"]
    },
    "ToHop": {
        "category": "baitap", 
        "subcategory": "tohop",
        "display_name": "T·ªï h·ª£p tuy·∫øn t√≠nh",
        "description": "B√†i t·∫≠p v·ªÅ t·ªï h·ª£p tuy·∫øn t√≠nh v√† ƒë·ªôc l·∫≠p tuy·∫øn t√≠nh",
        "difficulty_level": "olympic",
        "keywords": ["b√†i t·∫≠p", "luy·ªán t·∫≠p", "d·∫°ng b√†i", "t·ªï h·ª£p tuy·∫øn t√≠nh", "ƒë·ªôc l·∫≠p tuy·∫øn t√≠nh", "linear combination"]
    },
    "DaThuc": {
        "category": "baitap",
        "subcategory": "dathuc", 
        "display_name": "ƒêa th·ª©c",
        "description": "B√†i t·∫≠p v·ªÅ ƒëa th·ª©c v√† kh√¥ng gian ƒëa th·ª©c",
        "difficulty_level": "olympic",
        "keywords": ["b√†i t·∫≠p", "luy·ªán t·∫≠p", "d·∫°ng b√†i", "ƒëa th·ª©c", "polynomial", "kh√¥ng gian ƒëa th·ª©c"]
    },
    "DT": {
        "category": "baitap",
        "subcategory": "dt",
        "display_name": "ƒê·ªãnh th·ª©c", 
        "description": "B√†i t·∫≠p v·ªÅ ƒë·ªãnh th·ª©c v√† t√≠nh ch·∫•t ƒë·ªãnh th·ª©c",
        "difficulty_level": "olympic",
        "keywords": ["b√†i t·∫≠p", "luy·ªán t·∫≠p", "d·∫°ng b√†i", "ƒë·ªãnh th·ª©c", "determinant", "det"]
    }
}

# C·∫•u h√¨nh
IMAGE_BASE_URL = "https://cdn.mathpix.com/"
ALL_CREATED_IDS = set()

# ========================
# UTILITY FUNCTIONS
# ========================

def get_category_info(filename):
    """L·∫•y th√¥ng tin category t·ª´ t√™n file"""
    filename_lower = filename.lower()
    
    logging.info(f"üîç Ph√¢n t√≠ch file: {filename}")
    
    # Ki·ªÉm tra ƒë·ªÅ thi
    if "dethi" in filename_lower:
        if "banga" in filename_lower:
            logging.info(f"   ‚Üí Ph√¢n lo·∫°i: ƒê·ªÅ thi B·∫£ng A")
            return CATEGORY_MAPPING["dethi_bangA"]
        elif "bangb" in filename_lower:
            logging.info(f"   ‚Üí Ph√¢n lo·∫°i: ƒê·ªÅ thi B·∫£ng B")
            return CATEGORY_MAPPING["dethi_bangB"]
        else:
            logging.warning(f"   ‚Üí ƒê·ªÅ thi kh√¥ng r√µ b·∫£ng, m·∫∑c ƒë·ªãnh B·∫£ng A")
            return CATEGORY_MAPPING["dethi_bangA"]
    
    # Ki·ªÉm tra b√†i t·∫≠p theo 7 d·∫°ng
    for topic_code in ["GTR", "HPT", "KGVT", "MT", "ToHop", "DaThuc", "DT"]:
        if topic_code.lower() in filename_lower:
            logging.info(f"   ‚Üí Ph√¢n lo·∫°i: B√†i t·∫≠p {CATEGORY_MAPPING[topic_code]['display_name']}")
            return CATEGORY_MAPPING[topic_code]
    
    # Default fallback
    logging.warning(f"   ‚Üí Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c, d√πng m·∫∑c ƒë·ªãnh")
    return {
        "category": "baitap",
        "subcategory": "general",
        "display_name": "B√†i t·∫≠p t·ªïng h·ª£p",
        "description": "B√†i t·∫≠p t·ªïng h·ª£p c√°c ch·ªß ƒë·ªÅ",
        "difficulty_level": "olympic",
        "keywords": ["b√†i t·∫≠p", "luy·ªán t·∫≠p", "d·∫°ng b√†i"]
    }

def replace_images_with_placeholders(text):
    """Thay th·∫ø h√¨nh ·∫£nh b·∫±ng placeholder v√† tr·∫£ v·ªÅ danh s√°ch URLs"""
    image_urls = re.findall(r'!\[.*?\]\((.*?)\)', text)
    if not image_urls:
        return text, []

    counter = itertools.count(1)
    
    def replacer(match):
        return f"[IMAGE_{next(counter)}]"

    modified_text = re.sub(r'!\[.*?\]\((.*?)\)', replacer, text)
    return modified_text, image_urls

def normalize_image_url(url):
    """Chu·∫©n h√≥a ƒë∆∞·ªùng d·∫´n ·∫£nh"""
    if url.startswith('/') or url.startswith('.\\') or url.startswith('./'):
        filename = os.path.basename(url)
        return f"{IMAGE_BASE_URL}{filename}"
    
    if url.startswith('http://') or url.startswith('https://'):
        return url
    
    return f"{IMAGE_BASE_URL}{url}"

def get_format_type(filename):
    """Nh·∫≠n di·ªán lo·∫°i file: 'exam' cho ƒë·ªÅ thi, 'topic' cho d·∫°ng b√†i t·∫≠p"""
    return "exam" if "dethi" in filename.lower() else "topic"

# ========================
# PARSING FUNCTIONS - K·∫øt h·ª£p t·ª´ c·∫£ 2 file g·ªëc
# ========================

def split_bai(text, format_type=None):
    """
    T√°ch c√°c b√†i l·ªõn trong file markdown
    K·∫øt h·ª£p logic t·ª´ c·∫£ 2 file g·ªëc ƒë·ªÉ x·ª≠ l√Ω m·ªçi tr∆∞·ªùng h·ª£p
    """
    # Chu·∫©n h√≥a d√≤ng
    text = re.sub(r'\r\n|\r', '\n', text)
    text = re.sub(r'(?<!\n)(B√†i\s+\d+\.)', r'\n\1', text)
    
    # T·ª± ƒë·ªông x√°c ƒë·ªãnh format_type n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p
    if format_type is None:
        if re.search(r'\nB√†i\s+\d+\.\d+', text):
            format_type = "topic"
        else:
            format_type = "exam"
    
    if format_type == "exam":
        # ƒê·ªÅ thi: B√†i 1., B√†i 2., ...
        pattern = r'\n(B√†i\s+\d+\.)([\s\S]*?)(?=\nB√†i\s+\d+\.|\Z)'
    else:
        # B√†i t·∫≠p: B√†i 5.1, B√†i 5.2, ...
        pattern = r'\n(B√†i\s+\d+\.\d+)([\s\S]*?)(?=\nB√†i\s+\d+\.\d+|\Z)'
    
    matches = re.findall(pattern, text)
    result = {}
    
    for m in matches:
        # m[0]: ti√™u ƒë·ªÅ b√†i, m[1]: n·ªôi dung
        match_num = re.match(r'B√†i\s*(\d+(?:\.\d+)?).*', m[0])
        key = match_num.group(1) if match_num else str(len(result)+1)
        # Gh√©p l·∫°i ti√™u ƒë·ªÅ + n·ªôi dung
        result[key] = (m[0] + '\n' + m[1]).strip()
    
    return result

def split_problem_parts(text):
    """
    T√°ch c√°c √Ω nh·ªè (a), (b), (c) trong m·ªôt b√†i
    Tr·∫£ v·ªÅ problem_statement v√† problem_parts ri√™ng bi·ªát
    """
    text = re.sub(r'\r\n|\r|\n', '\n', text)
    
    # T√¨m √Ω ƒë·∫ßu ti√™n ƒë·ªÉ t√°ch problem_statement
    first_part_match = re.search(r'(?:^|[\n\.])\s*\(([a-hj-zA-HJ-Z])\)\s*[:\.]?\s*', text)
    
    if not first_part_match:
        # Kh√¥ng c√≥ √Ω con, to√†n b·ªô l√† problem_statement
        return text.strip(), {}
    
    # T√°ch problem_statement (ph·∫ßn tr∆∞·ªõc √Ω ƒë·∫ßu ti√™n)
    problem_statement = text[:first_part_match.start()].strip()
    
    # T√°ch c√°c √Ω con
    pattern = r'(?:^|[\n\.])\s*\(([a-hj-zA-HJ-Z])\)\s*[:\.]?\s*'
    matches = list(re.finditer(pattern, text, flags=re.DOTALL))
    
    problem_parts = {}
    for idx, m in enumerate(matches):
        key = m.group(1).lower()
        start = m.end()
        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(text)
        content = text[start:end].strip()
        
        # Lo·∫°i b·ªè d·∫•u : ho·∫∑c . ƒë·∫ßu d√≤ng n·∫øu c√≤n s√≥t
        while content and content[0] in ":. ":
            content = content[1:].strip()
        
        problem_parts[key] = content
    
    return problem_statement, problem_parts

def split_solution_parts(text):
    """
    T√°ch l·ªùi gi·∫£i th√†nh c√°c ph·∫ßn t∆∞∆°ng ·ª©ng v·ªõi c√°c √Ω a, b, c
    """
    text = re.sub(r'\r\n|\r|\n', '\n', text)
    
    # T√¨m pattern cho l·ªùi gi·∫£i t·ª´ng √Ω
    pattern = r'(?:^|[\n\.])\s*\(([a-hj-zA-HJ-Z])\)\s*[:\.]?\s*'
    matches = list(re.finditer(pattern, text, flags=re.DOTALL))
    
    if not matches:
        # Kh√¥ng c√≥ √Ω con, to√†n b·ªô l√† l·ªùi gi·∫£i chung
        return text.strip(), {}
    
    # L·ªùi gi·∫£i chung (ph·∫ßn tr∆∞·ªõc √Ω ƒë·∫ßu ti√™n)
    full_solution = text[:matches[0].start()].strip()
    
    # T√°ch l·ªùi gi·∫£i t·ª´ng √Ω
    solution_parts = {}
    for idx, m in enumerate(matches):
        key = m.group(1).lower()
        start = m.end()
        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(text)
        content = text[start:end].strip()
        
        # Lo·∫°i b·ªè d·∫•u : ho·∫∑c . ƒë·∫ßu d√≤ng n·∫øu c√≤n s√≥t
        while content and content[0] in ":. ":
            content = content[1:].strip()
        
        solution_parts[key] = content
    
    return full_solution, solution_parts

def extract_school_info(text):
    """Tr√≠ch xu·∫•t th√¥ng tin tr∆∞·ªùng ƒë·∫°i h·ªçc t·ª´ text"""
    school_pattern = r'\(([^)]*(?:ƒêH|ƒê·∫°i h·ªçc|University)[^)]*)\)'
    matches = re.findall(school_pattern, text, re.IGNORECASE)
    return matches[0] if matches else ""

def extract_question_number(text, format_type=None):
    """Tr√≠ch xu·∫•t s·ªë c√¢u h·ªèi t·ª´ text"""
    if format_type == "exam":
        match = re.search(r'B√†i\s+(\d+)', text)
        return match.group(1) if match else "1"
    else:
        match = re.search(r'B√†i\s+([\d\.]+)', text)
        return match.group(1) if match else "1"

# ƒê√£ lo·∫°i b·ªè c√°c function t·ª± sinh th√¥ng tin kh√¥ng ch√≠nh x√°c:
# - auto_tags(): T·∫°o tags kh√¥ng ƒë√°ng tin c·∫≠y
# - extract_concepts(): Mapping c·ª©ng kh√¥ng ph·∫£n √°nh n·ªôi dung th·ª±c

# ========================
# MAIN PROCESSING FUNCTION
# ========================

def create_problem_object_new_structure(bt_content, lg_content, metadata, category_info):
    """
    T·∫°o object b√†i to√°n v·ªõi c·∫•u tr√∫c JSON m·ªõi - t√°ch bi·ªát ƒë·ªÅ b√†i v√† l·ªùi gi·∫£i
    """
    # X·ª≠ l√Ω h√¨nh ·∫£nh
    bt_clean, bt_images = replace_images_with_placeholders(bt_content)
    lg_clean, lg_images = replace_images_with_placeholders(lg_content)
    
    # Chu·∫©n h√≥a URLs h√¨nh ·∫£nh
    all_images = []
    for i, url in enumerate(bt_images + lg_images):
        all_images.append({
            "id": f"IMAGE_{i+1}",
            "url": normalize_image_url(url),
            "description": f"H√¨nh ·∫£nh {i+1}",
            "position": "in_content" if i < len(bt_images) else "in_solution"
        })
    
    # T√°ch ƒë·ªÅ b√†i th√†nh statement v√† parts
    problem_statement, problem_parts = split_problem_parts(bt_clean)
    
    # T√°ch l·ªùi gi·∫£i th√†nh full solution v√† parts
    full_solution, solution_parts = split_solution_parts(lg_clean)
    
    # Lo·∫°i b·ªè t·∫•t c·∫£ th√¥ng tin t·ª± sinh kh√¥ng ch√≠nh x√°c
    # concepts = extract_concepts(bt_content + " " + lg_content, category_info)
    # tags = auto_tags(bt_content + " " + lg_content, category_info)
    
    # T·∫°o object theo c·∫•u tr√∫c m·ªõi
    problem_object = {
        "id": metadata["id"],
        "category": category_info["category"],
        "subcategory": category_info["subcategory"],
        
        # Metadata cho k·ª∑ y·∫øu ƒë·∫°i s·ªë tuy·∫øn t√≠nh
        "metadata": {
            "year": metadata["year"],
            "source_file": metadata["source"],
            "subject": "dai_so_tuyen_tinh",
            "document_type": "ky_yeu",
            "category_name": category_info["display_name"],
            "difficulty": category_info["difficulty_level"],
            "created_at": datetime.datetime.now().isoformat()
        },
        
        # N·ªôi dung b√†i to√°n (t√°ch bi·ªát)
        "problem_statement": problem_statement,
        "problem_parts": problem_parts,
        "images": all_images,
        
        # L·ªùi gi·∫£i (t√°ch bi·ªát)
        "solution": {
            "full_solution": full_solution,
            "solution_parts": solution_parts
        },
        
        # Lo·∫°i b·ªè educational_info t·ª± sinh kh√¥ng ch√≠nh x√°c
        
        # Th√¥ng tin b·ªï sung
        "title": metadata["title"],
        "question_number": metadata["question"],
        "source_path": metadata.get("source_md", "")
    }
    
    return problem_object

def process_one_pair_final(bt_path, lg_path, year, category_info, output_json_path, id_prefix="2024"):
    """
    X·ª≠ l√Ω m·ªôt c·∫∑p file BT + LG th√†nh JSON v·ªõi c·∫•u tr√∫c m·ªõi ho√†n ch·ªânh
    """
    logging.info(f"üìÑ X·ª≠ l√Ω c·∫∑p: {os.path.basename(bt_path)} + {os.path.basename(lg_path)}")
    
    try:
        # ƒê·ªçc file BT v√† LG
        with open(bt_path, 'r', encoding='utf-8') as f:
            bt_content = f.read()
        
        with open(lg_path, 'r', encoding='utf-8') as f:
            lg_content = f.read()
        
        # X√°c ƒë·ªãnh format type
        format_type = get_format_type(os.path.basename(bt_path))
        
        # T√°ch c√°c b√†i
        bt_problems = split_bai(bt_content, format_type)
        lg_solutions = split_bai(lg_content, format_type)
        
        logging.info(f"   üìù T√¨m th·∫•y {len(bt_problems)} b√†i t·∫≠p, {len(lg_solutions)} l·ªùi gi·∫£i")
        
        # Gh√©p b√†i t·∫≠p v·ªõi l·ªùi gi·∫£i
        problems = []
        for ma_bai in bt_problems:
            bt_problem = bt_problems.get(ma_bai, "")
            lg_solution = lg_solutions.get(ma_bai, "")
            
            if not bt_problem.strip():
                continue
            
            # Tr√≠ch xu·∫•t metadata
            school_info = extract_school_info(bt_problem)
            question_num = extract_question_number(bt_problem, format_type)
            
            # T·∫°o ID unique
            base_name = os.path.splitext(os.path.basename(bt_path))[0].lower()
            problem_id = f"{year}-{id_prefix}-{base_name}-{ma_bai.replace('.', '')}"
            
            # Ki·ªÉm tra tr√πng l·∫∑p ID
            if problem_id in ALL_CREATED_IDS:
                counter = 1
                while f"{problem_id}-{counter}" in ALL_CREATED_IDS:
                    counter += 1
                problem_id = f"{problem_id}-{counter}"
            
            ALL_CREATED_IDS.add(problem_id)
            
            # T·∫°o title
            if format_type == "exam":
                title = f"ƒê·ªÅ thi Olympic {year} - B√†i {ma_bai} ({category_info['subcategory'].upper()})"
            else:
                title = f"B√†i {ma_bai}: {category_info['display_name']}"
            
            # Metadata
            metadata = {
                "id": problem_id,
                "title": title,
                "year": year,
                "exam": "Olympic" if format_type == "exam" else "T·ªïng h·ª£p",
                "question": ma_bai,
                "source_school": f"{school_info} {year}" if school_info else "",
                "source": os.path.basename(bt_path),
                "source_md": bt_path
            }
            
            # T·∫°o object b√†i to√°n v·ªõi c·∫•u tr√∫c m·ªõi
            problem_obj = create_problem_object_new_structure(
                bt_content=bt_problem,
                lg_content=lg_solution,
                metadata=metadata,
                category_info=category_info
            )
            
            problems.append(problem_obj)
        
        # L∆∞u JSON
        os.makedirs(os.path.dirname(output_json_path), exist_ok=True)
        with open(output_json_path, 'w', encoding='utf-8') as f:
            json.dump(problems, f, ensure_ascii=False, indent=2)
        
        logging.info(f"‚úÖ ƒê√£ l∆∞u {len(problems)} b√†i v√†o {output_json_path}")
        return len(problems)
        
    except Exception as e:
        logging.error(f"‚ùå L·ªói x·ª≠ l√Ω {bt_path}: {str(e)}")
        return 0

def auto_process_all_final(root_dir="data/raw", output_root="data/processed/final"):
    """
    T·ª± ƒë·ªông x·ª≠ l√Ω t·∫•t c·∫£ file MD v·ªõi c·∫•u tr√∫c JSON m·ªõi ho√†n ch·ªânh
    """
    logging.info("üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω t·∫•t c·∫£ file MD v·ªõi c·∫•u tr√∫c JSON m·ªõi...")
    total_files_processed = 0
    total_problems_created = 0
    
    # X√≥a t·∫≠p h·ª£p ID to√†n c·ª•c tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu
    ALL_CREATED_IDS.clear()
    
    # X·ª≠ l√Ω th∆∞ m·ª•c 2024, 2023, 2018 (c√≥ th·ªÉ m·ªü r·ªông cho c√°c nƒÉm kh√°c)
    for year_folder in ["2024", "2023", "2018"]:
        year_path = os.path.join(root_dir, year_folder)
        if not os.path.exists(year_path):
            continue
            
        bt_dir = os.path.join(year_path, "BT")
        lg_dir = os.path.join(year_path, "LG")
        
        if not (os.path.isdir(bt_dir) and os.path.isdir(lg_dir)):
            logging.warning(f"‚ö†Ô∏è B·ªè qua {year_folder}: kh√¥ng t√¨m th·∫•y c·∫•u tr√∫c BT/LG")
            continue
            
        logging.info(f"üìÇ ƒêang x·ª≠ l√Ω nƒÉm {year_folder}...")
        bt_files = [f for f in os.listdir(bt_dir) if f.endswith(".md")]
        lg_files = [f for f in os.listdir(lg_dir) if f.endswith(".md")]
        
        # T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c output theo category
        output_year_dir = os.path.join(output_root, year_folder)
        os.makedirs(output_year_dir, exist_ok=True)
        
        for bt_file in bt_files:
            # L·∫•y th√¥ng tin category
            category_info = get_category_info(bt_file)
            
            # T√¨m file l·ªùi gi·∫£i t∆∞∆°ng ·ª©ng v·ªõi logic linh ho·∫°t cho m·ªçi nƒÉm
            base_name = bt_file.replace("BT_", "").replace(".md", "")
            lg_file = None
            
            # T√¨m file LG t∆∞∆°ng ·ª©ng v·ªõi pattern kh√°c nhau theo nƒÉm
            for lf in lg_files:
                # Pattern nƒÉm 2024: LG_BT_DaThuc_2024.md
                # Pattern nƒÉm 2018: LG_DaThuc_2018.md
                # Pattern ƒë·ªÅ thi: LG_dethi_bangA_2024.md (gi·ªëng nhau)
                
                if f"LG_BT_{base_name}" in lf:  # Pattern 2024 cho b√†i t·∫≠p
                    lg_file = lf
                    break
                elif f"LG_{base_name}" in lf:  # Pattern 2018 ho·∫∑c ƒë·ªÅ thi
                    lg_file = lf
                    break
            
            if not lg_file:
                logging.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y l·ªùi gi·∫£i cho {bt_file}")
                continue
                
            bt_path = os.path.join(bt_dir, bt_file)
            lg_path = os.path.join(lg_dir, lg_file)
            
            # T·∫°o ƒë∆∞·ªùng d·∫´n output theo category
            category_dir = os.path.join(output_year_dir, category_info["category"], category_info["subcategory"])
            os.makedirs(category_dir, exist_ok=True)
            
            output_filename = f"{base_name}.json"
            output_json_path = os.path.join(category_dir, output_filename)
            
            # X·ª≠ l√Ω c·∫∑p file
            problems_count = process_one_pair_final(
                bt_path=bt_path,
                lg_path=lg_path,
                year=int(year_folder),
                category_info=category_info,
                output_json_path=output_json_path,
                id_prefix=year_folder
            )
            
            if problems_count > 0:
                total_files_processed += 1
                total_problems_created += problems_count
    
    # T·∫°o summary report
    create_processing_summary(output_root, total_files_processed, total_problems_created)
    
    logging.info(f"‚úÖ Ho√†n t·∫•t x·ª≠ l√Ω:")
    logging.info(f"   üìÅ {total_files_processed} c·∫∑p file ƒë√£ x·ª≠ l√Ω")
    logging.info(f"   üìù {total_problems_created} b√†i to√°n ƒë√£ t·∫°o")
    logging.info(f"   üíæ D·ªØ li·ªáu l∆∞u trong: {output_root}")
    
    return total_files_processed, total_problems_created

def create_processing_summary(output_root, total_files, total_problems):
    """T·∫°o b√°o c√°o t·ªïng k·∫øt qu√° tr√¨nh x·ª≠ l√Ω"""
    summary = {
        "processing_info": {
            "timestamp": datetime.datetime.now().isoformat(),
            "total_files_processed": total_files,
            "total_problems_created": total_problems,
            "output_directory": output_root
        },
        "data_structure": {
            "schema_version": "2.0",
            "features": [
                "Separated problem statement and solution",
                "Hierarchical categorization (category + subcategory)",
                "Rich metadata for filtering",
                "Educational information",
                "Image handling with placeholders",
                "Backward compatibility"
            ]
        },
        "categories": {
            "dethi": {
                "subcategories": ["bangA", "bangB"],
                "description": "Olympic exam problems"
            },
            "baitap": {
                "subcategories": ["gtr", "hpt", "kgvt", "mt", "tohop", "dathuc", "dt"],
                "description": "Exercise problems by topic"
            }
        }
    }
    
    summary_path = os.path.join(output_root, "processing_summary.json")
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    logging.info(f"üìä ƒê√£ t·∫°o b√°o c√°o t·ªïng k·∫øt: {summary_path}")

if __name__ == "__main__":
    # Ch·∫°y x·ª≠ l√Ω t·ª± ƒë·ªông v·ªõi c·∫•u tr√∫c m·ªõi
    total_files, total_problems = auto_process_all_final()
    
    logging.info("üéâ Ho√†n th√†nh x·ª≠ l√Ω v·ªõi c·∫•u tr√∫c JSON m·ªõi!")
    logging.info("üìã C·∫•u tr√∫c d·ªØ li·ªáu:")
    logging.info("   ‚úì T√°ch bi·ªát ƒë·ªÅ b√†i v√† l·ªùi gi·∫£i")
    logging.info("   ‚úì Ph√¢n lo·∫°i 2 c·∫•p (category + subcategory)")
    logging.info("   ‚úì Metadata ƒë·∫ßy ƒë·ªß cho chatbot")
    logging.info("   ‚úì Th√¥ng tin gi√°o d·ª•c b·ªï sung")
    logging.info("   ‚úì Backward compatibility")