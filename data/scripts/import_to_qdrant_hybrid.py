#!/usr/bin/env python3
"""
Import d·ªØ li·ªáu v√†o Qdrant Cloud v·ªõi chi·∫øn l∆∞·ª£c HYBRID
- Filter ch√≠nh x√°c + Semantic search
- Metadata ƒë·∫ßy ƒë·ªß cho filtering
- Embedding text t·ªëi ∆∞u (gi·ªØ nguy√™n LaTeX + context)
"""

import os
import json
import logging
import time
import re
import hashlib
from tqdm import tqdm
from dotenv import load_dotenv
from openai import OpenAI
from qdrant_client import QdrantClient
from qdrant_client.http import models
import glob

# Load environment variables from backend/.env
load_dotenv("backend/.env")

# C·∫•u h√¨nh t·ª´ environment variables
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
QDRANT_COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME", "math_collection")

# Kh·ªüi t·∫°o clients
client = OpenAI(api_key=OPENAI_API_KEY)
qdrant_client = QdrantClient(
    url=QDRANT_URL,
    api_key=QDRANT_API_KEY,
)

# C·∫•u h√¨nh
EMBEDDING_MODEL = "text-embedding-3-small"
VECTOR_SIZE = 1536

# Import Smart Translator
import sys
sys.path.append('data/scripts')
from smart_latex_translator import SmartLatexTranslator

# Kh·ªüi t·∫°o translator
smart_translator = SmartLatexTranslator(client)

def create_numeric_id(string_id):
    """T·∫°o numeric ID t·ª´ string ID ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi Qdrant Cloud"""
    # S·ª≠ d·ª•ng hash ƒë·ªÉ t·∫°o ID s·ªë nguy√™n duy nh·∫•t
    hash_object = hashlib.md5(string_id.encode())
    # L·∫•y 8 bytes ƒë·∫ßu v√† convert th√†nh int
    numeric_id = int.from_bytes(hash_object.digest()[:8], byteorder='big')
    # ƒê·∫£m b·∫£o l√† s·ªë d∆∞∆°ng
    return abs(numeric_id)

def translate_latex_to_natural(latex_text, context=""):
    """D·ªãch LaTeX th√†nh vƒÉn b·∫£n t·ª± nhi√™n b·∫±ng Smart Translator"""
    
    if not latex_text or not latex_text.strip():
        return latex_text
    
    try:
        # S·ª≠ d·ª•ng Smart Translator
        translated = smart_translator.translate(latex_text, context)
        return translated
        
    except Exception as e:
        print(f"‚ùå L·ªói d·ªãch LaTeX: {e}")
        return latex_text

def create_enhanced_embedding_text(item):
    """T·∫°o text cho embedding - T·∫≠p trung v√†o ƒë·ªÅ b√†i v·ªõi LaTeX translation"""
    
    parts = []
    
    # === METADATA CONTEXT ===
    parts.append(f"Ch·ªß ƒë·ªÅ: {item['metadata']['category_name']}")
    parts.append(f"NƒÉm: {item['metadata']['year']}")
    parts.append(f"Lo·∫°i: {item['category']} {item['subcategory']}")
    
    # === N·ªòI DUNG ƒê·ªÄ B√ÄI (D·ªäCH LaTeX) - TR·ªåNG T√ÇM ===
    context = f"{item['metadata']['category_name']} - {item['category']}"
    problem_natural = translate_latex_to_natural(item['problem_statement'], context)
    parts.append(f"ƒê·ªÅ b√†i: {problem_natural}")
    
    # Th√™m problem parts (d·ªãch)
    for part_key, part_content in item["problem_parts"].items():
        part_natural = translate_latex_to_natural(part_content, context)
        parts.append(f"({part_key}) {part_natural}")
    
    # === TITLE ===
    parts.append(item['title'])
    
    # === TH√äM CONTEXT T·ª™ L·ªúI GI·∫¢I (NH·∫∏) ===
    # Ch·ªâ th√™m 1 d√≤ng t√≥m t·∫Øt ƒë·ªÉ c·∫£i thi·ªán context
    if item.get('solution', {}).get('full_solution'):
        solution_preview = item['solution']['full_solution'][:100] + "..."
        solution_natural = translate_latex_to_natural(solution_preview, context)
        parts.append(f"Ph∆∞∆°ng ph√°p: {solution_natural}")
    
    return "\n".join(parts)

def get_embedding(text):
    """T·∫°o semantic embedding t·ª´ OpenAI"""
    try:
        response = client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"‚ùå L·ªói t·∫°o embedding: {e}")
        return None

def create_keyword_vector(text):
    """T·∫°o keyword vector (sparse) cho hybrid search - Simple TF-IDF approach"""
    
    # Simple keyword extraction cho Vietnamese math terms
    import re
    from collections import Counter
    
    # Normalize text
    text_lower = text.lower()
    
    # Extract important math keywords
    math_keywords = [
        'ma tr·∫≠n', 'matrix', 'ƒë·ªãnh th·ª©c', 'det', 'h·∫°ng', 'rank',
        'ph∆∞∆°ng tr√¨nh', 'equation', 'h·ªá', 'system', 'nghi·ªám', 'solution',
        'gi√° tr·ªã ri√™ng', 'eigenvalue', 'vector ri√™ng', 'eigenvector',
        'kh√¥ng gian', 'space', 'tuy·∫øn t√≠nh', 'linear', 'ƒë·ªôc l·∫≠p', 'independent',
        't√≠ch ph√¢n', 'integral', 'ƒë·∫°o h√†m', 'derivative', 'gi·ªõi h·∫°n', 'limit',
        't·ªï h·ª£p', 'combination', 'ƒëa th·ª©c', 'polynomial'
    ]
    
    # Simple keyword scoring
    keyword_scores = {}
    for i, keyword in enumerate(math_keywords):
        count = text_lower.count(keyword)
        if count > 0:
            keyword_scores[i] = float(count)
    
    # Convert to sparse vector format (simple approach)
    # Note: Trong production, n√™n d√πng SPLADE ho·∫∑c BM25
    return keyword_scores if keyword_scores else None

def create_collection():
    """T·∫°o collection v·ªõi MULTI-VECTOR schema theo khuy·∫øn ngh·ªã Gemini"""
    
    collection_name = QDRANT_COLLECTION_NAME  # S·ª≠ d·ª•ng t√™n t·ª´ .env
    
    try:
        # Ki·ªÉm tra collection c≈© (KH√îNG x√≥a t·ª± ƒë·ªông ƒë·ªÉ tr√°nh m·∫•t d·ªØ li·ªáu)
        try:
            existing = qdrant_client.get_collection(collection_name)
            print(f"‚ö†Ô∏è Collection {collection_name} ƒë√£ t·ªìn t·∫°i. S·∫Ω th√™m d·ªØ li·ªáu v√†o collection hi·ªán c√≥.")
        except:
            print(f"üìù T·∫°o collection m·ªõi: {collection_name}")
        
        # T·∫°o collection v·ªõi MULTI-VECTOR configuration (ch·ªâ khi ch∆∞a t·ªìn t·∫°i)
        try:
            qdrant_client.get_collection(collection_name)
            print(f"‚úÖ Collection {collection_name} ƒë√£ t·ªìn t·∫°i, b·ªè qua t·∫°o m·ªõi")
        except:
            qdrant_client.create_collection(
                collection_name=collection_name,
                vectors_config={
                    # Vector d√†y cho semantic search
                    "semantic_vector": models.VectorParams(
                        size=VECTOR_SIZE,  # 1536 for text-embedding-3-small
                        distance=models.Distance.COSINE
                    ),
                    # Vector th∆∞a cho keyword matching (s·∫Ω implement sau)
                    # "keyword_vector": models.SparseVectorParams()
                }
            )
            print(f"‚úÖ ƒê√£ t·∫°o collection m·ªõi: {collection_name}")
        
        # T·∫°o RICH PAYLOAD INDEXES theo Gemini Table 2.1.1 + problem_section
        payload_indexes = [
            # Core identifiers
            ("doc_id", models.PayloadSchemaType.KEYWORD),
            ("source_file", models.PayloadSchemaType.KEYWORD),
            
            # Filtering fields
            ("category", models.PayloadSchemaType.KEYWORD),
            ("subcategory", models.PayloadSchemaType.KEYWORD),
            ("metadata.year", models.PayloadSchemaType.INTEGER),
            ("metadata.difficulty", models.PayloadSchemaType.KEYWORD),
            ("metadata.subject", models.PayloadSchemaType.KEYWORD),
            
            # Question filtering (NEW)
            ("question_number", models.PayloadSchemaType.KEYWORD),
            ("problem_section", models.PayloadSchemaType.KEYWORD),
            
            # Full-text search fields
            ("latex_string", models.PayloadSchemaType.TEXT),
            ("natural_language_desc", models.PayloadSchemaType.TEXT),
        ]
        
        for field_name, field_type in payload_indexes:
            try:
                qdrant_client.create_payload_index(
                    collection_name=collection_name,
                    field_name=field_name,
                    field_schema=field_type
                )
                print(f"   ‚úÖ Created index: {field_name} ({field_type})")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Index {field_name}: {e}")
        
        print(f"‚úÖ ƒê√£ t·∫°o collection: {collection_name}")
        print("‚úÖ ƒê√£ t·∫°o indexes cho filtering")
        
    except Exception as e:
        print(f"‚ùå L·ªói t·∫°o collection {collection_name}: {e}")
        return False
    
    return True

def extract_problem_section(question_number: str, category: str) -> str:
    """Extract problem section from question_number based on category."""
    if not question_number:
        return ""
    
    if category == "dethi":
        # For dethi: section = question_number (e.g., "1" -> "1")
        return question_number.strip()
    elif category == "baitap":
        # For baitap: section = part before dot (e.g., "1.2" -> "1")
        if "." in question_number:
            return question_number.split(".")[0].strip()
        else:
            return question_number.strip()
    else:
        return question_number.strip()


def upload_item_to_qdrant(item):
    """Upload item v·ªõi MULTI-VECTOR schema v√† RICH PAYLOAD + problem_section"""
    
    try:
        # === T·∫†O SEMANTIC VECTOR ===
        embedding_text = create_enhanced_embedding_text(item)
        semantic_vector = get_embedding(embedding_text)
        
        if semantic_vector is None:
            return False
        
        # === D·ªäCH TO√ÄN B·ªò N·ªòI DUNG ===
        context = f"{item['metadata']['category_name']} - {item['category']}"
        
        # D·ªãch ƒë·ªÅ b√†i
        problem_natural = translate_latex_to_natural(item['problem_statement'], context)
        
        # D·ªãch problem parts
        problem_parts_natural = {}
        for part_key, part_content in item["problem_parts"].items():
            problem_parts_natural[part_key] = translate_latex_to_natural(part_content, context)
        
        # D·ªãch l·ªùi gi·∫£i
        solution_natural = {}
        if item.get('solution', {}).get('full_solution'):
            solution_natural['full_solution'] = translate_latex_to_natural(
                item['solution']['full_solution'], context
            )
        
        # D·ªãch solution parts
        solution_parts_natural = {}
        for part_key, part_content in item.get('solution', {}).get('solution_parts', {}).items():
            solution_parts_natural[part_key] = translate_latex_to_natural(part_content, context)
        
        if solution_parts_natural:
            solution_natural['solution_parts'] = solution_parts_natural
        
        # === TH√äM PROBLEM_SECTION ===
        question_number = item.get("question_number", "")
        category = item.get("category", "")
        problem_section = extract_problem_section(question_number, category)
        
        # === T·∫†O RICH PAYLOAD theo Gemini Table 2.1.1 + problem_section ===
        payload = {
            # Core identifiers (theo Gemini schema)
            "doc_id": item["id"],
            "source_file": item["metadata"]["source_file"],
            
            # Filtering fields
            "category": item["category"],
            "subcategory": item["subcategory"],
            "metadata": item["metadata"],  # Gi·ªØ nguy√™n ƒë·ªÉ backward compatibility
            
            # LaTeX content (cho full-text search)
            "latex_string": item["problem_statement"],
            "natural_language_desc": problem_natural,
            
            # Complete content (gi·ªØ nguy√™n structure c≈©)
            "title": item["title"],
            "question_number": item["question_number"],
            "source_path": item["source_path"],
            "problem_statement": item["problem_statement"],
            "problem_parts": item["problem_parts"],
            "solution": item["solution"],
            
            # Enhanced natural language content
            "problem_statement_natural": problem_natural,
            "problem_parts_natural": problem_parts_natural,
            "solution_natural": solution_natural,
            
            # NEW: Problem section for advanced filtering
            "problem_section": problem_section,
            
            # Search optimization
            "content_type": "hybrid_v3",  # Updated version
            "embedding_text": embedding_text  # ƒê·ªÉ debug v√† analysis
        }
        
        # === T·∫†O NUMERIC ID cho Qdrant Cloud ===
        numeric_id = create_numeric_id(item["id"])
        
        # === UPLOAD V√ÄO QDRANT v·ªõi MULTI-VECTOR ===
        qdrant_client.upsert(
            collection_name=QDRANT_COLLECTION_NAME,  # S·ª≠ d·ª•ng collection name t·ª´ .env
            points=[{
                "id": numeric_id,  # S·ª≠ d·ª•ng numeric ID
                "vector": {
                    "semantic_vector": semantic_vector
                    # "keyword_vector": keyword_vector  # S·∫Ω implement sau
                },
                "payload": payload
            }]
        )
        
        return True
        
    except Exception as e:
        print(f"‚ùå L·ªói upload item {item['id']}: {e}")
        return False

def load_and_upload_data():
    """Load v√† upload t·∫•t c·∫£ d·ªØ li·ªáu JSON"""
    
    # T√¨m t·∫•t c·∫£ file JSON
    json_pattern = "data/processed/final/**/*.json"
    json_files = glob.glob(json_pattern, recursive=True)
    
    # Lo·∫°i b·ªè file summary
    json_files = [f for f in json_files if "processing_summary.json" not in f]
    
    print(f"üìÅ T√¨m th·∫•y {len(json_files)} file JSON")
    
    total_items = 0
    success_count = 0
    
    for json_file in tqdm(json_files, desc="X·ª≠ l√Ω files"):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"\nüìÑ X·ª≠ l√Ω file: {json_file}")
            print(f"   üìä S·ªë items: {len(data)}")
            
            for item in tqdm(data, desc="Upload items", leave=False):
                total_items += 1
                if upload_item_to_qdrant(item):
                    success_count += 1
                else:
                    print(f"‚ùå L·ªói upload: {item['id']}")
                
                # Delay nh·ªè ƒë·ªÉ tr√°nh rate limit
                time.sleep(0.1)
                
        except Exception as e:
            print(f"‚ùå L·ªói x·ª≠ l√Ω file {json_file}: {e}")
    
    print(f"\nüéâ HO√ÄN TH√ÄNH!")
    print(f"üìä T·ªïng items: {total_items}")
    print(f"‚úÖ Upload th√†nh c√¥ng: {success_count}")
    print(f"‚ùå L·ªói: {total_items - success_count}")
    
    return success_count

def test_search_functionality():
    """Test ch·ª©c nƒÉng t√¨m ki·∫øm v·ªõi filter"""
    
    print("\nüß™ TEST SEARCH FUNCTIONALITY")
    
    test_queries = [
        {
            "query": "ƒë·ªÅ thi b·∫£ng A nƒÉm 2024",
            "expected_filters": {
                "category": "dethi",
                "subcategory": "bangA", 
                "year": 2024
            }
        },
        {
            "query": "b√†i t·∫≠p ma tr·∫≠n nƒÉm 2018",
            "expected_filters": {
                "category": "baitap",
                "subcategory": "mt",
                "year": 2018
            }
        }
    ]
    
    for test in test_queries:
        print(f"\nüîç Query: {test['query']}")
        
        # T·∫°o filter conditions
        must_conditions = []
        for key, value in test['expected_filters'].items():
            if key == "year":
                must_conditions.append({
                    "key": "metadata.year",
                    "match": {"value": value}
                })
            else:
                must_conditions.append({
                    "key": key,
                    "match": {"value": value}
                })
        
        query_filter = {"must": must_conditions} if must_conditions else None
        
        try:
            # T√¨m ki·∫øm v·ªõi filter tr√™n MULTI-VECTOR collection
            query_vector = get_embedding(test['query'])
            results = qdrant_client.search(
                collection_name=QDRANT_COLLECTION_NAME,
                query_vector=("semantic_vector", query_vector),  # Ch·ªâ ƒë·ªãnh vector name
                query_filter=query_filter,
                limit=5,
                with_vectors=False  # Kh√¥ng c·∫ßn tr·∫£ v·ªÅ vectors ƒë·ªÉ ti·∫øt ki·ªám bandwidth
            )
            
            print(f"   üìä K·∫øt qu·∫£: {len(results)} items")
            for i, result in enumerate(results[:3]):
                payload = result.payload
                print(f"   {i+1}. {payload['title']} (Score: {result.score:.3f})")
                print(f"      Category: {payload['category']}/{payload['subcategory']}")
                print(f"      Year: {payload['metadata']['year']}")
                
        except Exception as e:
            print(f"   ‚ùå L·ªói search: {e}")

def load_and_upload_data():
    """Load v√† upload t·∫•t c·∫£ d·ªØ li·ªáu JSON"""
    
    # T√¨m t·∫•t c·∫£ file JSON
    json_files = glob.glob("data/processed/final/**/*.json", recursive=True)
    json_files = [f for f in json_files if "processing_summary.json" not in f]
    
    print(f"üìÅ T√¨m th·∫•y {len(json_files)} files")
    
    total_success = 0
    total_items = 0
    
    for json_file in json_files:
        print(f"\nüìÑ X·ª≠ l√Ω: {json_file}")
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                items = json.load(f)
            
            print(f"   üìä {len(items)} items")
            total_items += len(items)
            
            # Upload t·ª´ng item
            for item in items:
                if upload_item_to_qdrant(item):
                    total_success += 1
                    print(f"   ‚úÖ {total_success}/{total_items} uploaded", end='\r')
                else:
                    print(f"   ‚ùå L·ªói upload item {item.get('id', 'unknown')}")
                
                # Rate limiting ƒë·ªÉ tr√°nh qu√° t·∫£i
                time.sleep(0.1)
                
        except Exception as e:
            print(f"‚ùå L·ªói x·ª≠ l√Ω file {json_file}: {e}")
    
    print(f"\nüìä T·ªïng k·∫øt: {total_success}/{total_items} items uploaded th√†nh c√¥ng")
    return total_success

def main():
    """H√†m main"""
    
    print("üöÄ B·∫ÆT ƒê·∫¶U IMPORT D·ªÆ LI·ªÜU V√ÄO QDRANT CLOUD")
    print("=" * 50)
    
    # Ki·ªÉm tra c·∫•u h√¨nh
    if not all([OPENAI_API_KEY, QDRANT_URL, QDRANT_API_KEY]):
        print("‚ùå Thi·∫øu c·∫•u h√¨nh environment variables!")
        return
    
    print(f"üîó QDRANT_URL: {QDRANT_URL}")
    print(f"üì¶ COLLECTION: {QDRANT_COLLECTION_NAME}")
    
    # T·∫°o collection
    if not create_collection():
        return
    
    # Upload d·ªØ li·ªáu
    success_count = load_and_upload_data()
    
    if success_count > 0:
        # Test search
        test_search_functionality()
        
        print(f"\nüéâ TH√ÄNH C√îNG! ƒê√£ upload {success_count} items v√†o Qdrant Cloud")
    else:
        print("\n‚ùå TH·∫§T B·∫†I! Kh√¥ng upload ƒë∆∞·ª£c item n√†o")

if __name__ == "__main__":
    main()
